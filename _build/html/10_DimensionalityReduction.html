

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Dimensionsreduktion &#8212; Datenanalyse und Datenmanagement</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '10_DimensionalityReduction';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Clustering" href="11_Clustering.html" />
    <link rel="prev" title="Regularisierung" href="09_Regularization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="00_Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/Logo_VWA.jpg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/Logo_VWA.jpg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_Introduction.html">
                    Datenanalyse und Datenmanagment
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_PythonIntroduction.html">Einführung in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_DataAccess.html">Zugang zu Daten</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_DescriptiveAnalysis.html">Deskriptive Analyse von Daten</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_SupervisedLearning.html">Die Analyse abhängiger Variablen</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_LinearRegression.html">Das multiple lineare Regressionmodell</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Classification.html">Klassifizierung</a></li>


<li class="toctree-l1"><a class="reference internal" href="07_ModellAccuracy.html">Generalisierung funktionaler Zusammenhänge</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_ModelComplexity.html">Modellkomplexität</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Regularization.html">Regularisierung</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Dimensionsreduktion</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_Clustering.html">Clustering</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F10_DimensionalityReduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/10_DimensionalityReduction.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Dimensionsreduktion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hauptkomponentenanalyse">Hauptkomponentenanalyse</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anzahl-der-hauptkomponenten">Anzahl der Hauptkomponenten</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="dimensionsreduktion">
<h1>Dimensionsreduktion<a class="headerlink" href="#dimensionsreduktion" title="Permalink to this heading">#</a></h1>
<p>Bisher haben wir uns ausschließlich mit Methoden und Aufgaben aus dem Bereich des überwachten Lernens (Supervised Learning) befasst. Charakteristisch für das überwachte Lernen ist die Grundvoraussetzung einer Zielvariable. Diese muss durch den Nutzer spezifiziert werden. Oftmals kommt sogar noch hinzu, dass der Nutzer die Beobachtungen vor dem Trainieren des Modells selbst labeln muss. Dies betrifft beispielsweise der Beurteilung, ob Produktrezensionen positiv oder negativ sind. Die Betrachtung beim überwachten Lernen ist immer eine bedingte Betrachtung. Bedingt auf die Informationen der unabhängigen Variablen, welche Erwartung resultiert für die abhängige Variable. In manchen Situationen ist die unbedingte Betrachtung aller Variablen das Ziel. Allgemein gesprochen, möchte man bei dieser Art der Analyse allgmeine Erkenntnisse über einen Datensatz erlangen, ohne sich auf eine Variable zu fokussieren. Dies ist Ziel des unüberwachten Lernens (Unsupervised Learing). Wie beim überwachten Lernen existieren verschiedene Fragestellungen und Methoden. Zwei gängige Aufgaben des unüberwachten Lernens ist die Dimensionsreduktion und das Clustering von Daten. Wir wollen uns in diesem Kapitel mit der Dimensionsreduktion beschäftigen ehe wir uns im Folgekapitel das Clustering ansehen.</p>
<p>Bei der Dimensionsreduktion geht es darum aus <span class="math notranslate nohighlight">\(p\)</span> Variablen eines Datensatzes <span class="math notranslate nohighlight">\(d\)</span> neue Variablen zu generieren, wobei <span class="math notranslate nohighlight">\(d&lt;p\)</span> ist und möglichst wenig Informationsgehalt der originalen Daten bei der Reduzierung der Variablen verloren gehen soll. Einsatzmöglichkeiten von Dimensionsreduktionstechniken sind vielfältig. Hat ein Datensatz mehr als <span class="math notranslate nohighlight">\(3\)</span> Variablen, können wir die Daten nicht mehr visualisieren. Entsprechend können Dimensionsreduktionstechniken zur Reduktion der Variablenanzahl auf <span class="math notranslate nohighlight">\(2\)</span> oder <span class="math notranslate nohighlight">\(3\)</span> Dimensionen eingesetzt werden, um die Daten zumindest in kompirmierter Form visualisieren zu können. Des weiteren haben Methoden des überwachten Lernens oft Probleme, allgemeine Zusammenhänge zwischen den unabhängigen Daten und der Zielvariable zu identifizieren, wenn die Anzahl der unabhängigen Variablen <span class="math notranslate nohighlight">\(p\)</span> sehr hoch ist. Entsprechend kann es oft zu einer Verbesserung der Prognosen von Modellen kommen, wenn die unabhängigen Variablen auf eine kleinere Anzahl an Variablen reduziert werden, und diese Variablen im Anschluss für das Trainieren des Modells für die Zielvariable zu verwenden. Weiter Anwendungsgebiete der Dimensionsreduktion sind die Identifikation von Ausreissern, dem Ersetzen fehlender Daten oder der Kombination der im nächsten Kapitel besprochenen Clustermethoden.</p>
<section id="hauptkomponentenanalyse">
<h2>Hauptkomponentenanalyse<a class="headerlink" href="#hauptkomponentenanalyse" title="Permalink to this heading">#</a></h2>
<p>Während wir auf unterschiedliche Arten der Dimensionsreduktion zurückgreifen können, wollen wir uns in diesem Kurs beispielhaft ausschließlich mit der Hauptkomponentenanalyse (Principal Component Analysis - PCA) befassen, die bis heute einen hohen Stellenwert hat. Ähnlich wie das lineare Regressionsmodell ist die PCA nicht in der Lage nicht-lineare Zusammenhänge zwischen den Variablen zu berückstichtigen. Dafür hat sie sehr interessante und nützliche mathematische Eigenschaften aufgrund derer sie bis heute weit verbreitet eingesetzt wird. Wie die PCA technisch funktioniert kann durch unterschiedliche Interpretationen erklärt werden. Ich möchte mich auf die Interpretation beschränken, die der Art wie Modelle des überwachten Lernens trainiert wurden, sehr nahe kommt. Gegeben sei ein Datensatz mit <span class="math notranslate nohighlight">\(n\)</span> Beobachtungen von <span class="math notranslate nohighlight">\(p\)</span> Variablen:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = 
\begin{pmatrix}
x_{11} &amp; ... &amp; x_{1p} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; ... &amp; x_{np} \\
\end{pmatrix}
\end{split}\]</div>
<p>Für die PCA ist die Variation aller Datenpunkte von Interesse, daher werden alle Variablen vorab durch Subtrahieren des jeweiligen Mittelwerts auf den Durchschnittswert <span class="math notranslate nohighlight">\(0\)</span> transformiert. Durch diese Transformation kann die gesamte Variation der Daten durch den Durchschnitt aller quadrierten Werte quantifiziert werden kann:</p>
<div class="math notranslate nohighlight">
\[
Var(X) = \frac{1}{n} \sum_{j = 1}^p \sum_{i = 1}^n x_{ij}^2
\]</div>
<p>Ziel der PCA ist es, <span class="math notranslate nohighlight">\(d\)</span> neue Variablen zu erzeugen, die möglichst viel dieser Variation erklären können. Erinnern Sie sich an die lineare Regression zurück, ist dies auf analoge Art ähnlich, da bei der Regression die Variation der tatsächlicher Realisierung möglichst gut durch die Prognose des Modells erklärt werden soll. Eine neue Variable <span class="math notranslate nohighlight">\(z_m\)</span>, <span class="math notranslate nohighlight">\(m = 1, ... d\)</span> zu erzeugen, wird in der PCA durch die lineare Kombination der bestehenden Variablen realisiert. Die Linearkombination ist durch:</p>
<div class="math notranslate nohighlight">
\[
z_m = \phi_{1m} x_1 + \phi_{2m} x_2 + ... + \phi_{pm} x_p
\]</div>
<p>gegeben. Haben wir beispielsweise nur zwei Variablen <span class="math notranslate nohighlight">\(x_1, x_2\)</span>, so könnte eine Variable <span class="math notranslate nohighlight">\(z_1\)</span> durch:</p>
<div class="math notranslate nohighlight">
\[
z_1 = \phi_{11} x_1 + \phi_{21} x_2  
\]</div>
<p>generiert werden. Bei <span class="math notranslate nohighlight">\(\phi_{11}, \phi_{21}\)</span> handelt es sich um Parameter, die anhand von Daten trainiert werden. Das Ziel der PCA ist es <span class="math notranslate nohighlight">\(z_1\)</span> so zu bilden, dass aus dieser einen Variablen die Originaldaten möglichst gut wieder erzeugt werden könnten. Bezeichnen wir den Parametervektor für dieses Beispiel <span class="math notranslate nohighlight">\(\boldsymbol{\phi}_1 = \begin{pmatrix} \phi_{11} &amp; \phi_{21}  \end{pmatrix}^T\)</span>, so ensteht für jede Beobachtung <span class="math notranslate nohighlight">\(\boldsymbol{x} = \begin{pmatrix} x_{1} &amp; x_{2} \end{pmatrix}^T\)</span> der Wert der neuen Variable <span class="math notranslate nohighlight">\(z_{1}\)</span> durch:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_{1} = 
\begin{pmatrix} x_{1} &amp; x_{2} \end{pmatrix}
\begin{pmatrix} \phi_{11} \\ \phi_{21} \end{pmatrix}
\end{split}\]</div>
<p>Multipliziert man diesen Wert erneut mit dem transponierten Parametervektor, resultieren wieder zwei Werte, die als prognostizierte Rückbildung der Originaldaten interpretiert werden kann.</p>
<div class="math notranslate nohighlight">
\[
\begin{pmatrix} \tilde{x}_{1} &amp; \tilde{x}_{1} \end{pmatrix} = 
z_{1} 
\begin{pmatrix} \phi_{11} &amp; \phi_{21} \end{pmatrix}
\]</div>
<p>Es lässt sich zeigen, dass die Variation der Daten dann möglichst gut erklärt wird, wenn die Abweichung zwischen den Originaldaten <span class="math notranslate nohighlight">\(X\)</span> und den aus den reduzierte Daten rückgebildeten Daten <span class="math notranslate nohighlight">\(\tilde{X}\)</span> möglichst gering ist. Schreiben wir dies für den allgmeinen Fall mit <span class="math notranslate nohighlight">\(p\)</span> sowie <span class="math notranslate nohighlight">\(d \leq p\)</span> dimensionsreduzierten Variablen und <span class="math notranslate nohighlight">\(n\)</span> Beobachtungen auf, so sind die Parameter des Modells gegeben durch:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Phi = 
\begin{pmatrix}
\phi_{11} &amp; \phi_{12} &amp; \cdots &amp; \phi_{1d} \\
\phi_{21} &amp; \phi_{22} &amp; \cdots &amp; \phi_{2d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{p1} &amp; \phi_{p2} &amp; \cdots &amp; \phi_{pd} \\
\end{pmatrix}
\end{split}\]</div>
<p>Jede Spalte ist der Loadingvektor für die <span class="math notranslate nohighlight">\(m\)</span>-te Hauptkomponente. Für die Loadingvektoren besteht bei der PCA die Restriktion, dass ihre Länge auf den Wert <span class="math notranslate nohighlight">\(1\)</span> normiert ist und sie unabhängig sind, man spricht bei diesen Eigenschaften von orthonormalen Vektoren. Die neuen Variablen, die auch als Hauptkomponenten Scores bezeichnet werden, resultieren aus der Multiplikation der Originaldaten mit den Loadingvektoren, dies bedeutet die Scores der Hauptkomponente <span class="math notranslate nohighlight">\(m\)</span> entstehen durch:</p>
<div class="math notranslate nohighlight">
\[
z_m = \phi_{1m} x_1 + \phi_{2m} x_2 + ... + \phi_{pm} x_p
\]</div>
<p>und die Scores von <span class="math notranslate nohighlight">\(d\)</span> Hauptkomponenten sind gegeben durch:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z = X \Phi = 
\begin{pmatrix}
z_{11} &amp; ... &amp; z_{1d} \\
\vdots &amp; \ddots &amp; \vdots \\
z_{n1} &amp; ... &amp; z_{nd} \\
\end{pmatrix}
\end{split}\]</div>
<p>Aus diesen Werten, können Werte der Originaldaten durch:</p>
<div class="math notranslate nohighlight">
\[
\tilde{X} = Z \Phi^T
\]</div>
<p>zurück gebildet werden. Da durch die Dimensionsreduktion Informationen der Originaldaten verloren gehen, gilt in nahezu allen Fällen <span class="math notranslate nohighlight">\(X \neq \tilde{X}\)</span>, jedoch ist die dimensionsreduzierte Form der Originaldaten, umso besser je geringer die Abstände zwischen <span class="math notranslate nohighlight">\(X\)</span> und <span class="math notranslate nohighlight">\(\tilde{X}\)</span> sind. Diesen Abstand kann man durch:</p>
<div class="math notranslate nohighlight">
\[
L \left(X, \Phi\right) = \frac{1}{n} \sum_{j = 1}^p \sum_{i = 1}^n \left(x_{ij} - \tilde{x}_{ij}\right)^2
\]</div>
<p>quantifizieren. Entspechen sind die Werte <span class="math notranslate nohighlight">\(\Phi\)</span> zu bevorzugen durch die <span class="math notranslate nohighlight">\(L \left(X, \Phi\right)\)</span> minimiert wird. Mit diesen Parametern werden die Variablen der Dimensionsreduktion <span class="math notranslate nohighlight">\(Z = X \Phi \)</span> final erzeugt.</p>
<p>Zur Veranschaulichung betrachten wir ein einfaches Beispiel mit zwei Datenpunkten und zwei Variablen:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = 
\begin{pmatrix}
0.50 &amp; -0.50 \\
-0.50 &amp; 0.50 
\end{pmatrix}
\end{split}\]</div>
<p>Wir versuchen die Parameter für die erste Hauptkomponente <span class="math notranslate nohighlight">\(\phi_{11}, \phi_{21}\)</span> zu finden. In der unteren Grafik sehen wir links die originalen Datenpunkte und die aus der ersten Hauptkomponente zurück gebildeten Punkte. Der Abstand für den jeweiligen Datenpunkt ist durch den grauen Pfeil gekennzeichnet. In der linken Grafik sehen wir noch relativ hohe Abweichung zwischen Originaldaten und den zurück gebildeten Werten. Mit Veränderung der Parameter sehen wir in der rechten Grafik, dass für dieses Beispiel die Originaldaten durch die erste Hauptkomponente perfekt nachgegildet werden können, wenn bei der Bestimmung der ersten Hauptkomponente die Parameter <span class="math notranslate nohighlight">\(\phi_{11} = -0.707, \phi_{21} = 0.707\)</span> verwendet werden. Diese Wahl minimiert <span class="math notranslate nohighlight">\(L \left(X, \Phi\right)\)</span>. Mit diesen Parametern sind die Werte der erste Hauptkomponente <span class="math notranslate nohighlight">\(z_{11} = - 0.707, z_{21} = 0.707\)</span>, womit der erste Punkt den Originaldatenpunkt <span class="math notranslate nohighlight">\(x_{11} = 0.5, x_{12} = -0.5\)</span> und der zweite Wert den Originaldatenpunkt <span class="math notranslate nohighlight">\(x_{21} = -0.5, x_{22} = 0.5\)</span> repräsentiert. Dieses Beispiel soll den Aspekt der Minimierung der Abstände zwischen <span class="math notranslate nohighlight">\(X\)</span> und <span class="math notranslate nohighlight">\(\tilde{X}\)</span> verdeutlichen. Mathematisch kann die Minimierung der PCA durch die Eigenvektorzerlegung oder die Singularwertzerlegung durchgeführt werden, welche in den gängigen Paketen hinterlegt sind.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">slope</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">slope</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span>
<span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">phi_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.707</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.707</span><span class="p">]])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
<span class="n">X_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">phi</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
<span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_tilde</span><span class="o">-</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">phi_optimal</span><span class="p">)</span>
<span class="n">X_tilde_optimal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Z_optimal</span><span class="p">,</span> <span class="n">phi_optimal</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
<span class="n">diff_optimal</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_tilde_optimal</span><span class="o">-</span><span class="n">X</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$X$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_tilde</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{X}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">diff</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diff</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\phi_</span><span class="si">{11}</span><span class="s2"> = 0.894, \phi_</span><span class="si">{21}</span><span class="s2"> = 0.447$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$X$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_tilde_optimal</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_tilde_optimal</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\tilde</span><span class="si">{X}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">phi_optimal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">phi_optimal</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">diff_optimal</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diff_optimal</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">diff_optimal</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">diff_optimal</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\phi_</span><span class="si">{11}</span><span class="s2"> = -0.707, \phi_</span><span class="si">{21}</span><span class="s2"> = 0.707$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/26369af08afe07ebd8bc56d0cf74fb4ab839fc22e3b83914ff03673f6e20fb75.png" src="_images/26369af08afe07ebd8bc56d0cf74fb4ab839fc22e3b83914ff03673f6e20fb75.png" />
</div>
</div>
<p>Für den allgemeinen Fall mit <span class="math notranslate nohighlight">\(p\)</span> Variablen, können <span class="math notranslate nohighlight">\(d \leq p\)</span> Hauptkomponenten identifziert werden. Eine nützliche Eigenschaft ist, dass die Hauptkomponenten immer in absteigender Reihenfolge den höchsten Anteil der gesamten Variation der Daten erklären können. Dies bedeutet, die Streung der ersten Hauptkomponente  in der Lage ist den höchsten Anteil der gesamten Streuung unter aller Hauptkomponenten zu erklären. Auch hier besteht erneut eine Analogie zum linearen Regressionsmodell, beim dem wir das Modell über das <span class="math notranslate nohighlight">\(R^2\)</span> beuerteilt haben, bei dem der Wert umso höher ist, je mehr Variation der abhängigen Variable durch die Prognosen des Modells erklärt werden kann. Der Anteil erklärter Varianz durch die Hauptkomponente <span class="math notranslate nohighlight">\(m\)</span> ist gegeben durch:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{i=1}^n z_{im}^2}{\sum_{j = 1}^p \sum_{i = 1}^n x_{ij}^2}
\]</div>
<p>Entspechend ist der kumlative Anteil erklärter Varianz durch <span class="math notranslate nohighlight">\(M\)</span> Hauptkomponenten gegeben durch:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sum_{m=1}^M \sum_{i=1}^n z_{im}^2}{\sum_{j = 1}^p \sum_{i = 1}^n x_{ij}^2}
\]</div>
<p>Zudem lässt sich zeigen, dass sich die gesamte Variation in den erklärbaren Anteil und den Fehler bei der Rückbildung der Originaldaten zusammensetzt:</p>
<div class="math notranslate nohighlight">
\[
\sum_{j = 1}^p \sum_{i = 1}^n x_{ij}^2 = \sum_{m=1}^M \sum_{i=1}^n z_{im}^2 + \sum_{j = 1}^p \sum_{i = 1}^n \left( x_{ij} - \sum_{m=1}^M z_{im} \phi_{jm} \right)^2
\]</div>
<p>Aus dieser Darstellung sehen wir, dass der kumlative Anteil erklärter Varianz durch:</p>
<div class="math notranslate nohighlight">
\[
1 - \frac{\sum_{j = 1}^p \sum_{i = 1}^n \left( x_{ij} - \sum_{m=1}^M z_{im} \phi_{jm} \right)^2}{\sum_{j = 1}^p \sum_{i = 1}^n x_{ij}^2}
\]</div>
<p>dargestellt werden kann, was sehr nahe an der Idee des <span class="math notranslate nohighlight">\(R^2\)</span> der linearen Regression ist.</p>
<section id="anzahl-der-hauptkomponenten">
<h3>Anzahl der Hauptkomponenten<a class="headerlink" href="#anzahl-der-hauptkomponenten" title="Permalink to this heading">#</a></h3>
<p>Mit dem kumlativen Anteil erklärter Varianz, kann eine Entscheidung bei der Frage wieviele Hauptkomponenten verwendet werden sollten, getroffen werden. Im Beispiel der unteren Zelle sehen wir den (kumulativen) Anteil erklärter Varianz mit steigender Anzahl an Hauptkomponenten für einen Datensatz von Renditen der Aktienkurse der letzten eineinhalb Jahre von <span class="math notranslate nohighlight">\(20\)</span> US Unternehmen. Wir sehen, dass bereits <span class="math notranslate nohighlight">\(10-15\)</span> Hauptkomponenten genügen, um einen Großteil der vollständigen Variation aller Renditen zu erklären. Welcher Wert des kumulativen erklärten Anteils optimal ist, kann nicht allgemein beantwortet werden. Es besteht immer eine Art Trade-Off, den es zu beachten gilt. Werden viele Hauptkomponenten verwendet, ist die dimensionsreduzierte Form der Daten tendenziell näher an den Originaldaten. Bei weniger Hauptkomponenten ist die dimensionsreduzierte Form der Daten tendenziell weiter von den Originaldaten entfernt, jedoch bezieht sich diese Form mehr auf die wesentlichen Zusammenhänge der Originaldaten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="n">returns</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../Daten/stock_returns.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="s2">&quot;Date&quot;</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">returns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">]])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;cumulative&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of components&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Explained variance ratio&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cab6c00f8777df56fcb6bbc88823e94431ca47503eaa6b578f4cabb57be65d4c.png" src="_images/cab6c00f8777df56fcb6bbc88823e94431ca47503eaa6b578f4cabb57be65d4c.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="09_Regularization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Regularisierung</p>
      </div>
    </a>
    <a class="right-next"
       href="11_Clustering.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Clustering</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hauptkomponentenanalyse">Hauptkomponentenanalyse</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#anzahl-der-hauptkomponenten">Anzahl der Hauptkomponenten</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>